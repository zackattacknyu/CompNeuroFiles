

\documentclass[conference]{IEEEtran}


\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Planning Paths for Rats using Actor-Critic Model}


\author{Zachary DeStefano}

% make the title area
\maketitle


\begin{abstract}
For this project, I attempted to see what paths a rat would take if the Morris Water Maze contained multiple rewards of varying value. The inspiration for this project was the layout of cities and the transportation networks used to connect them. I hoped to observe if a trained rat would use paths that look similar to those transportation networks. In order to do this, I trained the rat using hippocampal place cells that learn via temporal difference learning in the actor-critic model. After training, I started the rat at random locations and recorded the paths it took to the reward centers. In the end, the paths are suboptimal thus rats should not be used for path planning. 
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}

The Morris Water Maze is one where a rat is released into a pool of water and searches for a platform which will provide relief from having to swim. The platform is thus considered a reward in the brain of the rat. Due to the constant location of these platforms, it is optimal for the rat to learn this location and it is desirable for us to model this learning.\\
\\
A common model for the neuronal activity in the rat involves place cells in the hippocampus that send signals depending on the current location of the rat. The Actor-Critic Model is used to control what the optimal next move should be for the rat. \\
\\
A common problem is transporation planning is that there are multiple locations that all need to reach each other so there needs to be a minimal cost network that connects them all. I attempted to see if rat movements could give more insights into this problem. \\

\section{Related Work}

\subsection{Foraging Behavior in Rats}
Foraging behavior for rats has been commonly studied.

\subsection{DA-STDP for foraging}

There were attempts to study foraging behavior using DA-STDP. In this paper, **CITE PLOS ONE PAPER**, they modeled a rat foraging for food in an environment. In the paper there were many issues getting the model to work thus it would have proven impractical for me to do. \\
\\
In this paper, **THE ROBOT**, they model a robot's movement via DA-STDP. While they accomplish a lot in the paper, due to the lack of place cells and other deficiencies with the input, it would have proven impractical for my purposes. Tuning DA-STDP would have been very difficult and deciding on good input parameters would also have been challenging. \\

\subsection{Slime Mode Interstates}

There was work done where food was placed at the location of the 20 largest cities in the United States and then slime mold was released. The paths sketched out by the slime mold ended up being similar to the interstate highway system. This is a great example of how the aggregate of simple actions taken by a biological organism can yield useful results. My goal with the rat experiment was to see if we would get useful results taking the aggregate of the paths it takes after learning. 


\section{Experiment Design}

The rat moves around a circular maze. Thus for my experiments there is a "rat" which has a discrete location. At any given moment, I made sure it is only allowed to continue walking in its direction or change its direction by 45 degrees. This is to more realistically model the momentum of the rat. Additionally, when it hits the wall of the maze, it "bounces off" in that the direction changes by 180 degrees.\\
\\
I trained the rat using temporal difference learning that is implemented via an actor-critic model. \\
\\
For the reward center locations, I used basic locations to start off and then used locations inspired by the cities in the United States and Europe. \\
\\
In order to have well placed cities, for my United States map, I used New York, Chicago, Los Angeles, Atlanta, and Denver. I found their latitude and longitude coordinates and plotted them on a 2D plane, rescaling for the water maze. \\
\\
For the Europe map, I used Paris, London, Warsaw, Berlin, and Vienna. I again used their latitude and longitude coordinates and put that on a 2D plane, rescaling as necessary. \\
\\
I used two models for rewards: constant rewards and diminishing rewards. In the constant reward model, every reward center gave the same amount and it did not diminish. In other model, the reward value diminished by a factor of 0.9 and it was not considered a reward if the value fell below 0.3. \\
\\
I also had the rewards diminish by a constant value until it fell below a threshold. **IMPLEMENT THIS**
\\
I had maps where every reward started with the same value and maps where they were different, with the differences being motivated by population of the cities. **IMPLEMENT THIS AND PUT RESULTS LATER**
\\
When training, the rat started from a few fixed locations. When testing, the rat would start at random locations. \\
\\
DETAILS ON ACTOR-CRITIC MODEL TRAINING USED\\
DETAILS ON LAYOUT AND REWARD VALUES USED\\
\\
I wanted to make sure that not too many trials were done during training. If too many are done, then the rat could have a heavy bias toward certain reward centers. Plus the value would diminish anyway after enough trials. Thus after each new map, I chose a new number of trials to do that training the rat well but kept the weights of the critic relatively equal across the board. \\
\\
One of the ways I knew the training was good was that the actor preferred directions pointed inward toward the reward centers instead of toward the walls as they had done during previous failed experiments

\section{Results}

One of the tradeoffs was that without sufficient training, the rat tends to hang around areas that are at an equal distance from the reward centers and cannot decide which direction to go in. \\
\\
FIGURES WITH THE PATH RESULTS AND LEARNED ACTOR-CRITIC VALUES\\
INCLUDE FIGURES SHOWING CUMULATIVE DISTANCE TO THE TARGET\\
\\
It can be reasonably assumed that the highest capacity highway would need to be placed where the rat's activity is the greatest. Indeed, the region with a high amount of activity is around the center of the city points. 

\section{Conclusion}
Rats should not be used for path planning


\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}




% that's all folks
\end{document}


